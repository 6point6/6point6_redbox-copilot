{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "import dotenv\n",
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.basename(os.getcwd()) != \"10ds-ai-redbox\":\n",
    "    os.chdir(\"..\")\n",
    "    print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv(\".env\")\n",
    "# Grab it as a dictionary too for convenience\n",
    "ENV = dotenv.dotenv_values(\".env\")\n",
    "\n",
    "model_params = {\"max_tokens\": 4096, \"temperature\": 0.2}\n",
    "\n",
    "llm = ChatAnthropic(\n",
    "    anthropic_api_key=ENV[\"ANTHROPIC_API_KEY\"],\n",
    "    max_tokens=model_params[\"max_tokens\"],\n",
    "    temperature=model_params[\"temperature\"],\n",
    "    streaming=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Judgement(BaseModel):\n",
    "    judgement: str = Field(\n",
    "        description=\"Comment on how good the answer is or not. If there is a mistake please explain what the mistake is.\"\n",
    "    )\n",
    "    correct: bool = Field(description=\"Whether the answer is correct or not\")\n",
    "\n",
    "\n",
    "class ReadingComprehensionOutput(BaseModel):\n",
    "    id: str = Field(description=\"ID of the question\")\n",
    "    question: str = Field(description=\"Question\")\n",
    "    llm_answer: str = Field(description=\"Answer\")\n",
    "    context: str = Field(description=\"Context\")\n",
    "    ground_truth_answers: List[str] = Field(description=\"Ground truth answers\")\n",
    "    retrieved_chunks_parent_ids: List[str] = Field(\n",
    "        description=\"Retrieved chunks parent IDs\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the outputs we'll be judging\n",
    "\n",
    "with open(\"./notebooks/results_SEED=12_DS=1000_k=4.json\", \"r\") as f:\n",
    "    outputs = json.load(f)[\"bad_outputs\"]\n",
    "\n",
    "    output_objects = []\n",
    "    for output_id, output in outputs.items():\n",
    "        output_objects.append(\n",
    "            ReadingComprehensionOutput(\n",
    "                id=output_id,\n",
    "                question=output[\"question\"],\n",
    "                llm_answer=output[\"llm_answer\"],\n",
    "                context=output[\"context\"],\n",
    "                ground_truth_answers=output[\"ground_truth_answers\"],\n",
    "                retrieved_chunks_parent_ids=output[\"retrieved_chunks_parent_ids\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Grab a sample for us to show the LLM how to mark\n",
    "few_shot_output_sample = output_objects[:5]\n",
    "for x in few_shot_output_sample:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(few_shot_output_sample[0].model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_judgement_0 = Judgement(\n",
    "    judgement=\"The context explicitly mentions that the School plays are normally fully booked every night. The answer is also overly verbose with a lot of unnecessary information about other documents.\",\n",
    "    correct=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(few_shot_output_sample[1].model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_judgement_1 = Judgement(\n",
    "    judgement=\"The context does say that it is free of charge almost all year around. This should be mentioned in the answer even if not all year round. The answer should be briefer and not mention unrelated information from the context.\",\n",
    "    correct=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(few_shot_output_sample[2].model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_judgement_2 = Judgement(\n",
    "    judgement=\"Correct time range extracted from the context. Good job is converting the jargony time units of Mega Annums into years. The answer is a bit verbose and could be shortened.\",\n",
    "    correct=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(few_shot_output_sample[3].model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_judgement_3 = Judgement(\n",
    "    judgement=\"The answer incorrectly covers the year of 1831 and not the 1837 mentioned in the question. Chopin visited London Incognito with Camille Pleyel. The answer is also overly verbose and could be shortened.\",\n",
    "    correct=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judgement_schema_str = json.dumps(Judgement.model_json_schema(), indent=4)\n",
    "\n",
    "# Using LLM to mark reading comprehension\n",
    "\n",
    "_reading_comprehension_template = \"\"\"\\\n",
    "You are a judge for reading comprehension accuracy. You will return as JSON \\\n",
    "formatted response for each item that will judge. The response will be \\\n",
    "formatted as follows (JSON Schema):\\n\\n{judgement_schema}\\n\\n\\\n",
    "Be objective in your judgement. If you are unsure, please mark as incorrect.\"\"\"\n",
    "\n",
    "JUDGEMENT_PROMPT = PromptTemplate.from_template(_reading_comprehension_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=JUDGEMENT_PROMPT.format(judgement_schema=judgement_schema_str)\n",
    "    ),\n",
    "    HumanMessage(content=few_shot_output_sample[0].model_dump_json(indent=4)),\n",
    "    AIMessage(content=example_judgement_0.model_dump_json(indent=4)),\n",
    "    HumanMessage(content=few_shot_output_sample[1].model_dump_json(indent=4)),\n",
    "    AIMessage(content=example_judgement_1.model_dump_json(indent=4)),\n",
    "    HumanMessage(content=few_shot_output_sample[2].model_dump_json(indent=4)),\n",
    "    AIMessage(content=example_judgement_2.model_dump_json(indent=4)),\n",
    "    HumanMessage(content=few_shot_output_sample[3].model_dump_json(indent=4)),\n",
    "    AIMessage(content=example_judgement_3.model_dump_json(indent=4)),\n",
    "    HumanMessage(content=few_shot_output_sample[4].model_dump_json(indent=4)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = llm(messages)\n",
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(few_shot_output_sample[4].model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
