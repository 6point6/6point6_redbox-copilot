{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Spotlight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will look into how to evaluate the spotlight.\n",
    "\n",
    "The first part of the spotlight summarises a document - the accuracy of this summary can be tested, using a ground truth summary and another LLM to do a comparison.\n",
    "\n",
    "This notebook will:\n",
    "* make an evaluation pipeline, which will take in a summary, a ground-truth summary, and then score how \"correct\" the summary is.\n",
    "* extract the summarisation pipeline from the app codebase, and test this pipeline against that.\n",
    "\n",
    "**This example relies on the \"``Guidance to civil servants on use of generative AI - GOV.UK.pdf``\" file having been loaded into the app on \"dev mode\".** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import textwrap\n",
    "from datetime import datetime\n",
    "\n",
    "import dotenv\n",
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.evaluation.qa.eval_chain import QAEvalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "import redbox.llm.llm_base as llm_base\n",
    "import redbox.llm.spotlight as spotlight\n",
    "from redbox.llm.prompts.spotlight import SPOTLIGHT_SUMMARY_TASK_PROMPT\n",
    "from redbox.models.file import File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv(\".env\")\n",
    "# Grab it as a dictionary too for convenience\n",
    "ENV = dotenv.dotenv_values(\".env\")\n",
    "model_params = {\"max_tokens\": 4096, \"temperature\": 0.2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatAnthropic(\n",
    "    anthropic_api_key=ENV[\"ANTHROPIC_API_KEY\"],\n",
    "    max_tokens=model_params[\"max_tokens\"],\n",
    "    temperature=model_params[\"temperature\"],\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "llm_handler = llm_base.LLMHandler(llm=llm, user_uuid=\"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spotlight summary\n",
    "We need to get three things:\n",
    "- the prompt used by the Spotlight summary\n",
    "- a summary of a document\n",
    "\n",
    "First we will make a function to generate the prompt, as it appears in the spotlight summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_info = {\n",
    "    \"name\": \"\",\n",
    "    \"email\": \"\",\n",
    "    \"department\": \"Cabinet Office\",\n",
    "    \"role\": \"Civil Servant\",\n",
    "    \"preffered_language\": \"British English\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spotlight_summary_prompt(file, prompt_template, user_info=user_info):\n",
    "    \"\"\"Generates a spotlight summary prompt for the supplied file and template\"\"\"\n",
    "    payload = f\"<Doc{file.uuid}>Title: {file.name}\\n\\n{file.text}</Doc{file.uuid}>\\n\\n\"\n",
    "\n",
    "    messages_to_send = [\n",
    "        SystemMessage(\n",
    "            content=prompt_template.format(\n",
    "                current_datetime=datetime.now().isoformat(),\n",
    "                user_info=user_info,\n",
    "            )\n",
    "        ),\n",
    "        HumanMessage(content=payload),\n",
    "    ]\n",
    "    return messages_to_send"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will get one of the documents, and prepare if for the splotlight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_to_civil_servants_json_path = (\n",
    "    \"data/dev/file/Guidance to civil servants on use of generative AI - GOV.UK.pdf.json\"\n",
    ")\n",
    "\n",
    "with open(guidance_to_civil_servants_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    guidance_to_civil_servants = File(**json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spotlight_summary(file, llm=llm, user_info=user_info):\n",
    "    \"\"\"Takes the supplied file and generates a spotlight summary\"\"\"\n",
    "    spotlight_model = spotlight.Spotlight(files=[file])\n",
    "\n",
    "    summary_task = spotlight.SpotlightTask(\n",
    "        id=\"summary\",\n",
    "        title=\"Summary\",\n",
    "        prompt_template=SPOTLIGHT_SUMMARY_TASK_PROMPT,\n",
    "    )\n",
    "\n",
    "    task_result = spotlight_model.run_task(\n",
    "        task=summary_task,\n",
    "        llm=llm,\n",
    "        user_info=user_info,\n",
    "    )\n",
    "    return task_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can assemble the actual prompt, the generated summary and a reference (ground truth summary), written by a human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_to_civil_servants_summary_prompt = generate_spotlight_summary_prompt(\n",
    "    guidance_to_civil_servants, SPOTLIGHT_SUMMARY_TASK_PROMPT\n",
    ")\n",
    "guidance_to_civil_servants_summary = spotlight_summary(\n",
    "    guidance_to_civil_servants\n",
    ").content\n",
    "guidance_to_civil_servants_reference_summary = \"\"\"\"AI can be helpful and assist your work and you are encouraged to explore this technology and consisder how it could be used in your organisation. However you must never put sensitive data into these tools and always be aware of how these systems can store, learn from and repeat what is given to them. The outputs from generative AI is susceptible to bias and misinformation and so should always be checked and cite\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation pipeline\n",
    "This is using the \"labelled criteria evaluator\" from langchain - reasonably simple to use, but we could always change this, or make our own.\n",
    "\n",
    "This takes in:\n",
    "- **input**: this is the question that was passed to the LLM being tested\n",
    "- **submission**: this is the answer the LLM being tested gave\n",
    "- **Criteria**: We will use \"correctness\", but there are others you can choose, such as \"conciseness\".\n",
    "- **Reference**: this is the ground truth answer to the \"input\" question, which the evaluator will compare the submission against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\n",
    "[BEGIN DATA]\n",
    "***\n",
    "[Input]: {input}\n",
    "***\n",
    "[Submission]: {output}\n",
    "***\n",
    "[Criteria]: {criteria}\n",
    "***\n",
    "[Reference]: {reference}\n",
    "***\n",
    "[END DATA]\n",
    "Does the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion \\\n",
    "to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only \\\n",
    "the single number from 0 to 9 (without quotes or punctuation) on its own line corresponding to score \\\n",
    "of whether the submission meets all criteria. At the end, repeat just the score again by itself on a new line.\"\"\"\n",
    "\n",
    "PROMPT_WITH_REFERENCES = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\", \"criteria\", \"reference\"], template=template\n",
    ")\n",
    "\n",
    "evaluator = load_evaluator(\n",
    "    \"labeled_criteria\", criteria=\"correctness\", llm=llm, prompt=PROMPT_WITH_REFERENCES\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple example of the evaluator being tested:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = evaluator.evaluate_strings(\n",
    "    input=guidance_to_civil_servants_summary_prompt,\n",
    "    prediction=guidance_to_civil_servants_summary,\n",
    "    reference=guidance_to_civil_servants_reference_summary,\n",
    ")\n",
    "print(eval_result[\"reasoning\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below prints the prompt, the model output and the reference, for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_print(text, width=150):\n",
    "    [print(line) for line in textwrap.wrap(str(text), width=width)]\n",
    "\n",
    "\n",
    "print(\"PROMPT: \", end=\"\")\n",
    "wrap_print(guidance_to_civil_servants_summary_prompt)\n",
    "print(\"----------------------------------------\")\n",
    "print(\"MODEL_OUTPUT: \", end=\"\")\n",
    "wrap_print(guidance_to_civil_servants_summary)\n",
    "print(\"----------------------------------------\")\n",
    "print(\"REFERENCE: \", end=\"\")\n",
    "wrap_print(guidance_to_civil_servants_reference_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simpler examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = evaluator.evaluate_strings(\n",
    "    input=\"What is the Royal Society's motto?\",\n",
    "    prediction=\"Nullius in verba\",\n",
    "    reference=\"The Royal Society's motto is 'Nullius in verba': take no ones word.\",\n",
    ")\n",
    "print(f'With ground truth: {eval_result[\"score\"]}')\n",
    "print(eval_result[\"reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = evaluator.evaluate_strings(\n",
    "    input=\"What are the main benefits of Reproducible Analytical Pipelines (RAP)?\",\n",
    "    prediction=\"Reproducible Analytical Pipelines allow for the automation of data processing and analysis, and through encouraging sharing and reuse of code, makes development more efficient.\",\n",
    "    reference=\"RAP results makes developmennt more efficient.\",\n",
    ")\n",
    "print(f'With ground truth: {eval_result[\"score\"]}')\n",
    "print(eval_result[\"reasoning\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
